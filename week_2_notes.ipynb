{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA, Validation, Data Leakages\n",
    "\n",
    "Patricia Schuster  \n",
    "Dec '19  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA (Exploratory Data Analysis)\n",
    "\n",
    "## What and why?\n",
    "\n",
    "Goal is to better understand the data in order to design powerful features and build accurate models. The best solutions incorporate unique insights on the data from EDA.\n",
    "\n",
    "Visualization allows us to immediately see the patterns. How can we use the patterns to build a better model?\n",
    "\n",
    "## Building intuition about the data\n",
    "\n",
    "* Getting domain knowledge  \n",
    "    * Usually don't need to go too deep inside the field  \n",
    "    * Read Wikipedia, google it, make sure you understand the data  \n",
    "    * Ex: Predict advertiser's cost. Look into Google's ad network.  \n",
    "* Check if data is intuitive, agree with domain knowledge?  \n",
    "    * Ex: An age column should have numbers 0-100. If you see an entry 336, it could indicate that the column is not human age, or there is a typo  \n",
    "    * Sometimes an \"error\" in the data is there due to how the data is exported. You may be able to use that.  \n",
    "* Understand how the data was generated  \n",
    "\n",
    "## Explore anonymized data\n",
    "\n",
    "* What is it?  \n",
    "    * Sometimes organizers will export data such that you can't get a specific value out of it. Ex: Doesn't want to reveal a document's content, so replace words with hash codes  \n",
    "* What can we do with it?   \n",
    "    * Look whether features are related to each other or grouped somehow  \n",
    "    * Run a basic ML like random forest and look at feature importances  \n",
    "    * Any features with mean near 0, std near 1? That may have been standard scaled by data processing.  \n",
    "    * Try to decode the feature / guess its true meaning / guess the feature type  \n",
    "\n",
    "\n",
    "## Exploration and visualization tools\n",
    "\n",
    "**First look**\n",
    "\n",
    "* Make several visualizations to investigate the same thing, so that you are not misled due to plot type  \n",
    "* Histogram with a big peak at the mean-- organizers may have filled empty data with the mean  \n",
    "* Use `pandas` `describe` function: `df.describe()`  \n",
    "* Evaluate number of occurrences of distinct pieces of data: `x.value_counts()`  \n",
    "* Look for missing data: `x.isnull()`  \n",
    "\n",
    "**Feature relations**\n",
    "\n",
    "* Scatter plot for all pairs of features: `pd.scatter_matrix(df)`  \n",
    "* Correlations: `df.corr()`  \n",
    "* How often is one feature larger than another? Build the matrix manually, `plt.matshow(_)`  \n",
    "* Calculate statistics of each feature, plot vs. index, sorted by the statistic: `df.mean().sort_values().plot(style='.')`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (A bit of) dataset cleaning\n",
    "\n",
    "* Unnecessary data: Feature data is constant  \n",
    "    * Original dataset might have been larger, and it was cut down for the competition  \n",
    "    * Find with `train.nunique(axis=1) == 1`  \n",
    "* Duplicated features  \n",
    "    * Remove duplicates. They won't add anything new and will consume memory.  \n",
    "    * Remove with `traintest.T.drop_duplicates()`  \n",
    "    * For categorical features, labels may be shuffled. We need to label encode the features separately for each column. Then they will look identical.   \n",
    "    * `for f in categorical feats:`  \n",
    "    *     `traintest[f] = traintest[f].factorize()`  \n",
    "    * `traintest.T.drop_duplicates()`  \n",
    "* Duplicate rows  \n",
    "    * Check if same rows have same label  \n",
    "    * Find duplicated rows, understand why they are duplicated. It may tell us something about dataset generation process.  \n",
    "* Check that the dataset is shuffled. If not, you may find data leakage. Plot target feature vs. row index.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation and overfitting\n",
    "\n",
    "Two main reasons why people may jump down the leaderboard after revealing private results:\n",
    "\n",
    "1) Competitors could ignore the validation and select the submission which scored best aginst the public leaderboard  \n",
    "2) Competitions may have no consistent public/private data split or they have too little data in either public or private leaderboard  \n",
    "\n",
    "Our task is to select our most appropriate submission to be evaluated by the private leaderboard.  \n",
    "\n",
    "Next videos:\n",
    "\n",
    "1) Understand validation and overfitting  \n",
    "2) Identify number of splis that should be done to establish stable validation   \n",
    "3) Break down most frequent ways to repeat train test split  \n",
    "4) Discuss most common validation problems  \n",
    "\n",
    "## What is validation?\n",
    "\n",
    "Validation helps evaluate the quality of a model and select the model that will perform best on unseen data. Underfitting happens when you don't capture enough patterns in the data. Overfitting happens when you capture noise or patterns which do not generalize to the test data.  \n",
    "\n",
    "In a normal competition, we don't have access to the test data. We don't want to overfit our method on the training data, so we usually split our training data further into a train and validation set. Fit the model on train, check it on validation.  \n",
    "\n",
    "Furthermore, often the organizers will split the test set into public and private data. The public score is what you receive back upon each submission, but it is being calculated on only a portion of the test data (usually 25-33%). The public leaderboard shows how your public score compares to other competitors.  \n",
    "\n",
    "When the competition ends, Kaggle scores your predictions against the remaining fraction of the test set, which is the private portion. You don't receive ongoing feedback about your private score-- it exists in the private leaderboard. Final competition results are based on the private leaderboard. This ensures the winner's results are accurate but generalized, without overfitting to the test data. \n",
    "\n",
    "Use cross validation and public leaderboard to optimize private leaderboard scores. \n",
    "\n",
    "## Validation strategies\n",
    "\n",
    "Three strategies, each with different number of splits:\n",
    "\n",
    "* Holdout  \n",
    "    * Splits into two  \n",
    "    * Samples in train and test do not overlap  \n",
    "    * Scheme  \n",
    "        * Split train data into `partA` and `partB`  \n",
    "        * Fit the model on `partA`, predict for `partB`  \n",
    "        * Use predictions for `partB` for estimating model quality. Find such hyper-parameters, that quality on `partB` is maximized  \n",
    "* K-fold  \n",
    "    * A repeated holdout  \n",
    "    * Create many folds, each with the same test-train split ratio, but using a different part of the data as test each time  \n",
    "    * Takes longer than holdout because you are essentially repeating holdout $K$ times  \n",
    "    * Scheme\n",
    "        * Split train data into K folds  \n",
    "        * Iterate through each fold: retrain the model on all folds except current fold, predict for the current fold  \n",
    "        * Use the predictions to calculate quality on each fold. Find hyper-parameters, such that quality on each fold is maximized. You can also estimate mean and variance of the loss. This is very helpful in order to understand significance of improvement  \n",
    "* Leave-one-out  \n",
    "    * A special case of K-fold, where $K$ is equal to the number of samples in the data  \n",
    "    * Iterate through every sample in the data as the test data  \n",
    "    * Scheme  \n",
    "        * Iterate over samples: retrain the model on all samples except current sample, predict for the current sample. You will need to retrain the model $N$ times (where $N$ is the number of samples in the dataset)  \n",
    "        * In the end you will get LOO predictions for every sample in the trainset and can calculate loss  \n",
    "    \n",
    "Usually use Holdout or K-fold on shuffle data.\n",
    "\n",
    "Stratification preserves the same target distribution over different folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data splitting strategies\n",
    "\n",
    "**We want to identify what train-test split strategy was used by the competition organizers and reproduce it.**\n",
    "\n",
    "How do we divide into train and test? Different splitting strategies can differ significantly in several ways:\n",
    "\n",
    "* In generated features  \n",
    "* In a way the model will rely on those features  \n",
    "* In some kind of target leak\n",
    "\n",
    "For a time-based trend, use a time-based split  \n",
    "\n",
    "* Everything before a given date as train, everything after as validation  \n",
    "* Testing data happens after our training set  \n",
    "* If we create features that are useful for a time-based split but not a random split, then validating on random split may not be useful. In time-based split, the validation data is forward in time, and therefore closer to the test data.   \n",
    "    \n",
    "Three general categories for splitting data:\n",
    "\n",
    "* Random, rowwise  \n",
    "    * The most common way of splitting data  \n",
    "    * Rows are fairly independent of each other  \n",
    "* Timewise  \n",
    "    * As described above  \n",
    "    * A special case for validation is a moving window validation. Shift the time range of data pulled for validation  \n",
    "* By ID  \n",
    "    * Probably no overlap  \n",
    "    * May have to reconstruct IDs  \n",
    "* Combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems that occur during validation\n",
    "\n",
    "Validation stage problems:  \n",
    "\n",
    "* Different scores for different train-test splits  \n",
    "    * Ex: Predicting sales. If you split based on time, Dec and Jan will have more sales than Feb based on holidays  \n",
    "    * What if there is no clear reason for different scores?  \n",
    "        * Too little data  \n",
    "        * Data too diverse and inconsistent  \n",
    "    * We should do extensive validation  \n",
    "        * Average scores from different K-fold splits  \n",
    "        * Tune model on one split, evaluate score on another  \n",
    "    \n",
    "Submission stage  \n",
    "\n",
    "* We may already have quite different scores in Kfold  \n",
    "* Too little data in public leaderboard  \n",
    "* Train and test data are from different distributions  \n",
    "* Trust your validation!  \n",
    "\n",
    "Leaderboard shuffle: Sometimes, after the competition ends, the leaders from the public leaderboard are shuffled in the private leaderboard. Why?  \n",
    "* Randomness  \n",
    "* Little amount of data  \n",
    "* Different public/private distributions  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data leakages\n",
    "\n",
    "Unexpected information in the data that allows us to be unrealistically good predictions. NOT OK to use in the real world, but maybe ok in competitions because you are trying to get the best predictions. \n",
    "\n",
    "Main types of data leaks:\n",
    "\n",
    "* Time-series  \n",
    "    * Features may contain information about the future  \n",
    "    * Weather, user history  \n",
    "* Unexpected information  \n",
    "    * Meta data from images  \n",
    "    * Information in IDs which is correlated to target variable  \n",
    "    * Row order  \n",
    "\n",
    "**Leaderboard probing**\n",
    "\n",
    "* Extracting ground truth from public parts of leaderboard  \n",
    "* Exploit vulnerabilities in public-private split  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
