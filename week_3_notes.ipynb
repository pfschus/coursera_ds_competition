{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 notes: Metrics optimization, advanced feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics optimization\n",
    "\n",
    "Metrics: Why are there so many? There are many ways to evaluate the quality of an analysis; we need to decide how to measure the quality. Example: A company will usually decide for itself what aspects of sales data are most important to it. \n",
    "\n",
    "If your model is scored with some metric, you should optimize exactly that metric to get best results. \n",
    "\n",
    "## Regression metrics review\n",
    "\n",
    "Notation for this lesson:\n",
    "\n",
    "- $N$ - number of objects  \n",
    "- $y \\in R^N$ - target values  \n",
    "- $\\hat{y} \\in R^N$ - predictions  \n",
    "- $\\hat{y}_i \\in R$ - prediction for i-th object  \n",
    "- $y_i \\in R$ - target for i-th object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most common metric: **MSE Mean Square Error**\n",
    "\n",
    "MSE = $\\frac{1}{N} \\sum_{i=1}^{N}(y_i-\\hat{y}_i)^2$  \n",
    "\n",
    "MSE = $\\frac{1}{N} \\sum_{i=1}^{N}(y_i-\\alpha)^2$,   \n",
    "where $\\alpha$ is the best constant target predictor, the target mean.  \n",
    "\n",
    "Grid search: Vary $\\alpha$ and see how MSE varies. Find $\\alpha$ with lowest MSE.  \n",
    "\n",
    "To optimize: Most libraries support it, so all you have to do is turn it on. Just find an example on GitHub to see how it's implemented. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another metric: **RMSE Root Mean Square Error**\n",
    "\n",
    "The square root is introduced to make the scale of the errors to be the same as the scale of the target. Now the error is linear, easier to comprehend. \n",
    "\n",
    "RMSE = $\\sqrt{\\frac{1}{N} \\sum_{i=1}^{N}(y_i-\\hat{y}_i)^2}$  \n",
    "\n",
    "Similar to MSE:  \n",
    "* Minimizers- any minimizer of MSE is a minimizer of RMSE  \n",
    "* If our target model is RMSE, we can still compare MSE or optimize based on MSE  \n",
    "* MSE is often easier to work with  \n",
    "\n",
    "Differences from MSE:  \n",
    "* Look at the gradient: Gradient of RMSE is the gradient of MSE times a value  \n",
    "* Means that- traveling along RMSE gradient is same as traveling along MSE gradient but with a different flow rate. The flow rate depends on the MSE score itself. Thus, they are not immediately interchangeable for gradient based methods. Adjust some parameters like the learning rate.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good for comparing model performance: **$R^2$**\n",
    "\n",
    "It's hard to determine if the model is good or not by looking at MSE or RMSE. We want to look at how much better the model is better than the baseline. For this reason, $R^2$ is often used.\n",
    "\n",
    "$R^2 = 1-\\frac{\\frac{1}{N} \\sum_{i=1}^{N}(y_i-\\hat{y}_i)^2}{\\frac{1}{N} \\sum_{i=1}^{N}(y_i-\\bar{y})^2} = 1 - 1-\\frac{MSE}{\\frac{1}{N} \\sum_{i=1}^{N}(y_i-\\bar{y})^2}$\n",
    "\n",
    "To optimize $R^2$, we can optimize MSE. This is because $R^2$ is basically MSE divided by a constant and subtracted from another constant. These constants don't matter for optimization. \n",
    "\n",
    "Another metric: **MAE Mean Absolute Error**\n",
    "\n",
    "The average of absolute differences between target values and the predictions.  \n",
    "$MAE = \\frac{1}{N}\\sum_{i=1}^N|y_i-\\hat{y}_i|$\n",
    "\n",
    "It penalizes huge errors less than MSE, so less sensitive outliers. Best constant predictor is target median.\n",
    "\n",
    "Different applications. MAE is widely used in finance, where a 10 dollar error is twice as bad as a 5 dollar error. (Compared to MSE which would make it four times as bad)\n",
    "\n",
    "To implement: not available in all models, but you can use similar loss models like Huber Loss, Quantile Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MAE vs. MSE**\n",
    "\n",
    "* Do you have outliers in the data? Use MAE  \n",
    "* Are you sure they are outliers? Use MAE  \n",
    "* Or are they just unexpected values we should still care about? Use MSE  \n",
    "\n",
    "(outliers result from measurement error, mistakes, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics with relative error: **(R)MSPE, MAPE, (R)MSLE**\n",
    "\n",
    "(R)MSPE: Like a weighted version of MSE  \n",
    "MSPE = $\\frac{100\\%}{N}\\sum_{i=1}^N( \\frac{y_i-\\hat{y}_i}{y_i}) ^2$  \n",
    "Best constant prediction is the weighted mean of the target values  \n",
    "\n",
    "MAPE: Like a weighted version of MAE  \n",
    "MAPE = $\\frac{100\\%}{N}\\sum_{i=1}^N |\\frac{y_i-\\hat{y}_i}{y_i}|$  \n",
    "Best constant prediction is the weighted median of the target values  \n",
    "\n",
    "(R)MSLE: Root mean square logarithmic error, RMSE calculated on a logarithmic scale.  \n",
    "RMSLE = $\\sqrt{ \\frac{1}{N}\\sum_{i=1}^N (\\log(y_i+1)-\\log(\\hat{y}_i+1)^2 }$\n",
    "Take log of target values and predictions and calculate RMSE between them.  \n",
    "Error curves are asymmetric.  \n",
    "Best constant prediction is target mean in log space\n",
    "\n",
    "To implement: Use weights for samples, `sample_weights` and use MSE or MAE. Or, resample the training set with the weights. For RMSLE, transform the target, fit the model with MSE loss, and then reverse transform the predictions back. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification metrics review\n",
    "\n",
    "Notation:\n",
    "\n",
    "$N$- number of objects  \n",
    "$L$- number of classes  \n",
    "$y$ - ground truth  \n",
    "$\\hat{y}$ - predictions  \n",
    "$|a=b|$ - indicator function  \n",
    "\"soft predictions\" - classifier's scores  \n",
    "\"hard predictions\" - a function of soft predictions, ex: a thresholding function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy score**: How frequently our class prediction is correct.\n",
    "Accuracy = $\\frac{1}{N}\\sum_{i=1}^N [\\alpha = y_i ]$  \n",
    "The best constant is to predict the most frequent class. \n",
    "\n",
    "**Logarithmic loss (logloss)**: Tries to get the classifier to output two posterior probabilities for their objects to be of a certain class.  \n",
    "(Binary) LogLoss = $\\frac{1}{N}\\sum_{i=1}^N y_i \\log (\\hat{y}_i) + (1-y_i)\\log (1-\\hat{y}_i)$  \n",
    "(Multiclass) LogLoss = $\\frac{-1}{N}\\sum_{i=1}^N\\sum_{i=1}^L y_{il} \\log(\\hat{y}_{il})$\n",
    "The elements $\\hat{y}_{il}$ are the probabilities of belonging to each of the classes. $\\hat{y}_{il}$ is a vector of length $L$ that sums to 1.   \n",
    "Logloss penalizes completely wrong answers and prefers to make a lot of small mistakes instead of one severe mistake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.3010299956639812"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "fx = 0.5\n",
    "yi = 0.5\n",
    "yi*np.log10(fx)+(1-yi)*np.log10(1-fx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.3010299956639812"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log10(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement: Most libraries have LogLoss method in them, just need to figure out which arguments should be passed to the library. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Area under curve (AUC ROC)**  \n",
    "- only for binary tasks  \n",
    "- Depends on ordering of the predictions, not on absolute values  \n",
    "- Performance measured by area under ROC curve  \n",
    "- Best constant prediction is 0.5  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cohen's Kappa motivation**\n",
    "\n",
    "* For accuracy of 1, it returns 1  \n",
    "* For baseline prediction, it returns 0  \n",
    "* This way you can't get misleadingly high scores for just guessing the baseline prediction  \n",
    "\n",
    "Cohen's Kappa = $1 - \\frac{1-\\text{accuracy}}{1-p_e}$\n",
    "\n",
    "* $p_e$- what accuracy would be on average, if we randomly permute our predictions  \n",
    "* $p_e = \\frac{1}{N^2}\\sum n_{k1}n_{k2}$\n",
    "\n",
    "Cohen's Kappa = $1 - \\frac{\\text{error}}{\\text{baseline error}}$\n",
    "\n",
    "*Weighted Kappa*\n",
    "\n",
    "For weighted error, multiply confusion matrix $C$ and weight matrix $W$ element-wise. Weighted error = $\\frac{1}{const}\\sum_{i,j} C_{i,j} W_{i,j} $  \n",
    "Weighted kappa = $1 - \\frac{\\text{weighted error}}{\\text{weighted baseline error}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General approaches for metrics optimization\n",
    "\n",
    "* *Target metric* is what we want to optimize  \n",
    "* *Optimization loss* is what the model optimizes  \n",
    "* It's easy to define a custom loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Encoding\n",
    " \n",
    "a.k.a \"target encoding\" or \"likelihood encoding\"  \n",
    "\n",
    "General idea: Add new variables based on some feature related to target. The simplest case is to encode each level of categorical variables with the corresponding target mean.  \n",
    "\n",
    "Frequency encoding: Instead of creating an arbitrary feature label, such as \"0,1,2,...\", encode each categorical feature with the corresponding mean target. Ex: If the target values for all rows with city \"Moscow\" are (0,1,1,0,0), you would encode \"Moscow\" with the feature mean 0.4. It's an easy concept, though there are pitfalls to be wary of.  \n",
    "\n",
    "Mean encoding helps to separate 0s from 1s, so classes look way more separable. In general, the more complicated and non linear the feature target dependency is, the more effective mean encoding is.  \n",
    "\n",
    "Datasets that could benefit from mean encoding often have categorical variables with a lot of levels. Regression tasks are more flexible for mean encoding than classification.    \n",
    "\n",
    "Ways to use target variable:\n",
    "\n",
    "* Likelihood = (goods)/(goods+bads) = mean(target)  \n",
    "* Weight of evidence = ln(goods/bads) x 100  \n",
    "* Count = Goods = sum(target)  \n",
    "* Diff = Goods - Bads  \n",
    "\n",
    "These can't be used as-is. We need to deal with overfitting first. Mean encodings require some kind of regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "Four kinds of regularization:  \n",
    "\n",
    "**1. CV (cross-validation) loop inside training data**  \n",
    "* Separate the data into k-node folds, usually 4-5 enough  \n",
    "* To get mean encoding values for some subset, we don't use data points from that subset; we estimate the encoding only on the rest of the subset  \n",
    "* Iteratively walk through all of the data subsets  \n",
    "    \n",
    "Take a look at their example:\n",
    "\n",
    "![CV loop](fig/reg_CV_loop_snippet.png)\n",
    "\n",
    "- `y_tr` is the target values in a Series  \n",
    "- `skr` generates the 5 folds. Each is a pair of indices for the `tr` and `val` arrays    \n",
    "- The loop iterates through the 5 folds. Within each fold, loop through columns  \n",
    "    - First, generate `X_tr`: used to estimate the encoding, and `X_val` use to apply the encoding  \n",
    "    - Calculate the means of each column in `X_val`, record in a new column  \n",
    "    - Fill the `train_new` data frame with the encoded result  \n",
    "    \n",
    "What's happening is, you are encoding each fold with the mean of values across the other four folds. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Smoothing**\n",
    "\n",
    "* If category is large, we can trusted the encoding because we have a lot of samples. But if the category is rare, then we can't.  \n",
    "* $\\alpha$ controls the amount of regularization  \n",
    "* Only works together with some other regularization method- can combine with CV loop\n",
    "\n",
    "Formula:  \n",
    "$\\frac{\\bar{x}_T n_{\\text{ROWS}} + \\bar{x}_G*\\alpha}{n_{\\text{ROWS}} + \\alpha}$\n",
    "\n",
    "$\\bar{x}_T$: Target mean  \n",
    "$\\bar{x}_G$: Global mean  \n",
    "$n_{\\text{ROWS}}$: Number of rows  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Adding random noise**\n",
    "\n",
    "Very finicky! Hard to make it work.\n",
    "\n",
    "**4. Sorting and calculating expanding mean**\n",
    "\n",
    "* Fix some sorting order for the data, and use only rows from zero to (n-1) to calculate the encoding for row n  \n",
    "* Upside: Least amount of leakage, no hyper parameter tuning   \n",
    "* Downside: Irregular encoding quality  \n",
    "* Built-in in CatBoost- performs great on datasets with categorical features    \n",
    "\n",
    "Code snippet:\n",
    "\n",
    "![CV loop](fig/expanding_mean_snippet.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `cumsum` stores the sum of the target variable up to the given row   \n",
    "* `cumcnt` stores the cumulative count  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
